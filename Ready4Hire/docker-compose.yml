version: '3.8'

services:
  # Ready4Hire API Service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ready4hire-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - LLM_PROVIDER=ollama
      - LLM_BASE_URL=http://ollama:11434
      - LOG_LEVEL=INFO
    volumes:
      - ./logs:/app/logs
      - ./app/datasets:/app/app/datasets
      - ./app/embeddings/cache:/app/app/embeddings/cache
    depends_on:
      - ollama
    networks:
      - ready4hire-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Nginx Reverse Proxy with SSL
  nginx:
    image: nginx:alpine
    container_name: ready4hire-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
      - ./app/static:/usr/share/nginx/html/static:ro
    depends_on:
      - api
    networks:
      - ready4hire-network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama LLM Service (optional, for local LLM)
  ollama:
    image: ollama/ollama:latest
    container_name: ready4hire-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - ready4hire-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Run this after container starts to pull model:
    # docker exec -it ready4hire-ollama ollama pull llama2

  # Redis (optional, for future caching/rate limiting)
  # redis:
  #   image: redis:7-alpine
  #   container_name: ready4hire-redis
  #   restart: unless-stopped
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis-data:/data
  #   networks:
  #     - ready4hire-network
  #   command: redis-server --appendonly yes

  # PostgreSQL (optional, for future persistent storage)
  # postgres:
  #   image: postgres:15-alpine
  #   container_name: ready4hire-db
  #   restart: unless-stopped
  #   environment:
  #     POSTGRES_DB: ready4hire
  #     POSTGRES_USER: ready4hire
  #     POSTGRES_PASSWORD: ${DB_PASSWORD}
  #   ports:
  #     - "5432:5432"
  #   volumes:
  #     - postgres-data:/var/lib/postgresql/data
  #   networks:
  #     - ready4hire-network

networks:
  ready4hire-network:
    driver: bridge

volumes:
  ollama-data:
  # redis-data:
  # postgres-data:
