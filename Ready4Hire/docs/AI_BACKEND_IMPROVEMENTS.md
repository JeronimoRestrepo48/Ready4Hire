# Mejoras Sugeridas para el Backend de IA

Basado en el an√°lisis de logs y c√≥digo, aqu√≠ est√°n las mejoras prioritarias:

## üî¥ Problemas Cr√≠ticos Detectados

### 1. **Timeout Configuraci√≥n Inconsistente**
**Problema:** 
- `config.py` tiene `OLLAMA_TIMEOUT: int = 30`
- `ollama_client.py` usa `timeout: int = 45` por defecto
- El cliente no est√° usando la configuraci√≥n de `Settings`

**Impacto:** Los logs muestran timeouts a 30s cuando deber√≠an ser 45s

**Soluci√≥n:**
```python
# En container.py, al inicializar OllamaClient:
from app.config import settings

self.llm_client = OllamaClient(
    timeout=settings.OLLAMA_TIMEOUT,  # Usar configuraci√≥n
    max_retries=settings.OLLAMA_MAX_RETRIES,
    ...
)
```

### 2. **Mapeo de Roles Incompleto**
**Problema:** 
- "Software Developer" no encuentra template (busca "software_developer" pero el template es "software_engineer")
- Varios nombres de roles no mapean correctamente

**Impacto:** Se usa template gen√©rico en vez de espec√≠fico

**Soluci√≥n:**
```python
# En advanced_prompts.py, mejorar _get_template_for_role:

ROLE_MAPPING = {
    "software developer": "software_engineer",
    "software engineer": "software_engineer",
    "developer": "software_engineer",
    "programmer": "software_engineer",
    "frontend developer": "frontend_developer",
    "backend developer": "backend_developer",
    # ... m√°s mapeos
}

def _get_template_for_role(self, role: str) -> PromptTemplate:
    role_lower = role.lower().strip()
    
    # Primero intentar mapeo directo
    if role_lower in ROLE_MAPPING:
        mapped_key = ROLE_MAPPING[role_lower]
        if mapped_key in self.templates:
            return self.templates[mapped_key]
    
    # Luego normalizar
    role_normalized = role_lower.replace(" ", "_").replace("-", "_")
    ...
```

### 3. **Error de Importaci√≥n `get_sync_session`**
**Problema:** 
- `main_v2_improved.py` intenta importar `get_sync_session` que no existe
- L√≠nea 181: `cannot import name 'get_sync_session'`

**Soluci√≥n:**
```python
# Verificar qu√© existe en postgres_sync_service.py
# O usar el m√©todo correcto de conexi√≥n
```

## üü° Mejoras de Rendimiento

### 4. **Circuit Breaker M√°s Resiliente**
**Mejora:** 
- Agregar health check peri√≥dico
- Mejor logging cuando el circuito est√° abierto
- Recovery autom√°tico m√°s inteligente

```python
# En ollama_client.py
def _check_health_periodic(self):
    """Verifica salud cada 30s cuando el circuito est√° abierto"""
    if self.circuit_breaker and self.circuit_breaker.state == CircuitState.OPEN:
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                logger.info("üü¢ Ollama recuperado, intentando cerrar circuito")
                # Forzar cierre del circuito
```

### 5. **Cache de Evaluaciones Mejorado**
**Mejora:**
- Cachear tambi√©n hints generados
- Cachear sanitizaci√≥n de respuestas
- Invalidaci√≥n inteligente por cambios en prompts

### 6. **Batch Processing para M√∫ltiples Evaluaciones**
**Mejora:**
- Si hay m√∫ltiples respuestas pendientes, procesarlas en batch
- Reducir overhead de conexiones

```python
async def batch_evaluate_answers(self, evaluations: List[Dict]) -> List[Dict]:
    """Procesa m√∫ltiples evaluaciones en paralelo"""
    tasks = [self.evaluate_answer(**eval_data) for eval_data in evaluations]
    return await asyncio.gather(*tasks, return_exceptions=True)
```

## üü¢ Mejoras de Calidad

### 7. **Mejor Manejo de Errores de Conexi√≥n**
**Mejora:**
- Detectar cuando Ollama se desconecta
- Reintentar con backoff exponencial m√°s agresivo
- Fallback a evaluaci√≥n heur√≠stica m√°s r√°pido

```python
# En ollama_client.py
def generate(self, ...):
    try:
        return self._generate_internal(...)
    except OllamaConnectionError as e:
        # Si es error de conexi√≥n, intentar health check
        if self._check_health():
            # Si Ollama est√° disponible, reintentar
            return self._generate_internal(...)
        else:
            # Ollama est√° ca√≠do, fallback inmediato
            raise OllamaUnavailableError("Ollama service is down")
```

### 8. **Validaci√≥n de Respuestas LLM Mejorada**
**Mejora:**
- Detectar cuando el LLM no responde JSON v√°lido
- Retry autom√°tico con prompt m√°s estricto
- Validaci√≥n de campos requeridos

```python
def _parse_evaluation_response(self, response: str, retry_on_fail: bool = True) -> Dict:
    """Parsea con retry autom√°tico si falla"""
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        if retry_on_fail:
            # Retry con prompt m√°s estricto
            logger.warning("JSON inv√°lido, reintentando con prompt m√°s estricto")
            return self._retry_with_strict_json_prompt(...)
        raise
```

### 9. **M√©tricas y Observabilidad**
**Mejora:**
- Tracking de latencia por tipo de evaluaci√≥n
- Contador de fallbacks heur√≠sticos
- M√©tricas de √©xito/fallo por rol

```python
class EvaluationMetrics:
    def __init__(self):
        self.total_evaluations = 0
        self.llm_success = 0
        self.fallback_count = 0
        self.latency_by_role = defaultdict(list)
    
    def record_evaluation(self, role: str, success: bool, latency: float, used_fallback: bool):
        self.total_evaluations += 1
        if success:
            self.llm_success += 1
        if used_fallback:
            self.fallback_count += 1
        self.latency_by_role[role].append(latency)
```

### 10. **Prompts M√°s Eficientes**
**Mejora:**
- Reducir tokens innecesarios en prompts
- Usar system prompts m√°s cortos
- Optimizar estructura JSON

### 11. **Sistema de Reintentos Inteligente**
**Mejora:**
- Detectar tipo de error (timeout vs conexi√≥n vs parse)
- Ajustar estrategia seg√∫n el error
- Timeout adaptativo seg√∫n complejidad de pregunta

## üìä Priorizaci√≥n

**Alta Prioridad (Cr√≠tico):**
1. ‚úÖ Fix timeout configuration
2. ‚úÖ Fix role mapping
3. ‚úÖ Fix get_sync_session import

**Media Prioridad (Importante):**
4. ‚úÖ Circuit breaker mejorado
5. ‚úÖ Mejor manejo de errores de conexi√≥n
6. ‚úÖ Validaci√≥n de respuestas mejorada

**Baja Prioridad (Mejora continua):**
7. ‚úÖ Batch processing
8. ‚úÖ M√©tricas avanzadas
9. ‚úÖ Prompts m√°s eficientes

## üöÄ Implementaci√≥n Sugerida

1. **Fase 1 (Urgente - 1 d√≠a):**
   - Fix configuraci√≥n timeout
   - Fix mapeo de roles
   - Fix importaci√≥n

2. **Fase 2 (Importante - 3 d√≠as):**
   - Mejorar circuit breaker
   - Mejorar manejo de errores
   - Validaci√≥n mejorada

3. **Fase 3 (Mejora continua - 1 semana):**
   - Batch processing
   - M√©tricas avanzadas
   - Optimizaciones de prompts

