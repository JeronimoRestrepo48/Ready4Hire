# üîß Ready4Hire - Gu√≠a de Resoluci√≥n de Problemas

## Tabla de Contenidos

1. [Problemas de Instalaci√≥n](#problemas-de-instalaci√≥n)
2. [Problemas con Ollama](#problemas-con-ollama)
3. [Problemas con la API](#problemas-con-la-api)
4. [Problemas con Tests](#problemas-con-tests)
5. [Problemas de Rendimiento](#problemas-de-rendimiento)
6. [Errores Comunes](#errores-comunes)
7. [Debugging Avanzado](#debugging-avanzado)

---

## Problemas de Instalaci√≥n

### ‚ùå Error: `ModuleNotFoundError: No module named 'app'`

**S√≠ntoma**:
```bash
$ uvicorn app.main_v2:app
ModuleNotFoundError: No module named 'app'
```

**Causas**:
- Ejecutando desde directorio incorrecto
- PYTHONPATH no configurado
- Estructura de paquetes incorrecta

**Soluci√≥n 1**: Ejecutar desde ra√≠z del proyecto

```bash
cd /path/to/Ready4Hire
python -m uvicorn app.main_v2:app --host 0.0.0.0 --port 8000
```

**Soluci√≥n 2**: Configurar PYTHONPATH

```bash
export PYTHONPATH="${PYTHONPATH}:/path/to/Ready4Hire"
uvicorn app.main_v2:app
```

**Soluci√≥n 3**: Usar script de inicio

```bash
./scripts/start_api.sh
```

---

### ‚ùå Error: `pip install` falla con dependencias

**S√≠ntoma**:
```bash
ERROR: Could not find a version that satisfies the requirement torch>=2.0.0
```

**Soluci√≥n 1**: Actualizar pip

```bash
python -m pip install --upgrade pip setuptools wheel
```

**Soluci√≥n 2**: Instalar dependencias por grupos

```bash
# Core dependencies
pip install fastapi uvicorn pydantic httpx

# ML dependencies
pip install torch transformers sentence-transformers

# Audio dependencies (opcional)
pip install openai-whisper pyttsx3
```

**Soluci√≥n 3**: Usar requirements espec√≠ficos

```bash
# Instalaci√≥n m√≠nima (sin audio)
pip install -r requirements.txt --no-deps
pip install -r requirements.txt

# Con cache de pip
pip install -r requirements.txt --cache-dir ~/.pip/cache
```

---

### ‚ùå Error: Permisos insuficientes

**S√≠ntoma**:
```bash
PermissionError: [Errno 13] Permission denied: '/usr/local/bin/ollama'
```

**Soluci√≥n**:

```bash
# Instalar con permisos de usuario
pip install --user -r requirements.txt

# O usar virtual environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## Problemas con Ollama

### ‚ùå Error: `Connection refused` al conectar con Ollama

**S√≠ntoma**:
```python
httpx.ConnectError: [Errno 111] Connection refused
```

**Diagn√≥stico**:

```bash
# Verificar si Ollama est√° corriendo
ps aux | grep ollama

# Verificar puerto
lsof -i :11434
```

**Soluci√≥n 1**: Iniciar Ollama

```bash
# Inicio manual
ollama serve > /tmp/ollama.log 2>&1 &

# Como servicio (Linux)
sudo systemctl start ollama
sudo systemctl enable ollama

# Verificar estado
curl http://localhost:11434/api/tags
```

**Soluci√≥n 2**: Verificar configuraci√≥n

```bash
# Ver variables de entorno
env | grep OLLAMA

# Configurar correctamente
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_ORIGINS="*"
```

---

### ‚ùå Error: Modelo no encontrado

**S√≠ntoma**:
```json
{"error": "model 'llama3.2:3b' not found"}
```

**Diagn√≥stico**:

```bash
# Listar modelos instalados
ollama list

# Verificar espacio en disco
df -h ~/.ollama
```

**Soluci√≥n**:

```bash
# Descargar modelo
ollama pull llama3.2:3b

# Alternativa con modelo m√°s peque√±o
ollama pull llama3.2:1b

# Verificar instalaci√≥n
ollama run llama3.2:3b "Hello"
```

---

### ‚ùå Error: Ollama muy lento o se queda colgado

**S√≠ntoma**:
- Respuestas tardan >60 segundos
- CPU al 100% constante
- OOM (Out of Memory)

**Diagn√≥stico**:

```bash
# Monitorear recursos
top -p $(pgrep ollama)

# Ver memoria disponible
free -h

# Ver uso de GPU (si aplica)
nvidia-smi
```

**Soluci√≥n 1**: Usar modelo m√°s peque√±o

```bash
# Modelo peque√±o (1-2GB RAM)
ollama pull llama3.2:1b
export OLLAMA_MODEL=llama3.2:1b

# Modelo mediano (4-8GB RAM)
ollama pull llama3.2:3b
export OLLAMA_MODEL=llama3.2:3b
```

**Soluci√≥n 2**: Limitar modelos cargados

```bash
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=1
ollama serve
```

**Soluci√≥n 3**: Ajustar par√°metros

En `.env`:
```bash
OLLAMA_MAX_TOKENS=256  # Reducir de 512
OLLAMA_TEMPERATURE=0.5  # Reducir de 0.7
```

---

## Problemas con la API

### ‚ùå Error: API no responde en el puerto esperado

**S√≠ntoma**:
```bash
$ curl http://localhost:8000/api/v2/health
curl: (7) Failed to connect to localhost port 8000
```

**Diagn√≥stico**:

```bash
# Ver procesos en puerto 8000
lsof -i :8000

# Ver logs de API
tail -f logs/ready4hire.log

# Verificar uvicorn
ps aux | grep uvicorn
```

**Soluci√≥n**:

```bash
# Matar proceso anterior
pkill -f uvicorn

# Iniciar API correctamente
cd /path/to/Ready4Hire
python -m uvicorn app.main_v2:app --host 0.0.0.0 --port 8000

# Verificar health endpoint
curl http://localhost:8000/api/v2/health
```

---

### ‚ùå Error 500: Internal Server Error

**S√≠ntoma**:
```json
{"detail": "Internal Server Error"}
```

**Diagn√≥stico**:

```bash
# Ver logs detallados
tail -100 logs/ready4hire.log

# Modo debug
LOG_LEVEL=DEBUG python -m uvicorn app.main_v2:app --reload
```

**Soluci√≥n 1**: Verificar dependencias

```python
# test_dependencies.py
from app.container import Container

try:
    container = Container()
    result = container.health_check()
    print(f"‚úÖ Container OK: {result}")
except Exception as e:
    print(f"‚ùå Error: {e}")
```

**Soluci√≥n 2**: Revisar configuraci√≥n

```bash
# Verificar .env existe
ls -la .env

# Verificar variables cr√≠ticas
grep -E "OLLAMA|API" .env
```

---

### ‚ùå Error: CORS bloqueando requests

**S√≠ntoma**:
```
Access to XMLHttpRequest blocked by CORS policy
```

**Soluci√≥n**:

En `app/main_v2.py`:

```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n: ["https://yourdomain.com"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## Problemas con Tests

### ‚ùå Tests fallan: `fixture 'container' not found`

**S√≠ntoma**:
```bash
E   fixture 'container' not found
```

**Soluci√≥n**:

Verificar `conftest.py` existe:

```python
# tests/conftest.py
import pytest
from app.container import Container

@pytest.fixture
def container():
    """Fixture que proporciona Container"""
    return Container()
```

---

### ‚ùå Tests fallan: Timeout al conectar con Ollama

**S√≠ntoma**:
```
httpx.ReadTimeout: Read operation timed out
```

**Soluci√≥n 1**: Iniciar Ollama antes de tests

```bash
# Iniciar Ollama
ollama serve > /tmp/ollama.log 2>&1 &
sleep 5

# Ejecutar tests
pytest tests/ -v
```

**Soluci√≥n 2**: Aumentar timeout en tests

```python
# tests/test_integration.py
import httpx

client = httpx.Client(
    base_url="http://localhost:11434",
    timeout=60.0  # Aumentar de 30 a 60 segundos
)
```

**Soluci√≥n 3**: Usar mocks

```python
# tests/mocks.py
class MockOllamaClient:
    def generate(self, prompt: str) -> str:
        return "Respuesta mock: Docker es una plataforma..."

@pytest.fixture
def mock_ollama(monkeypatch):
    monkeypatch.setattr("app.infrastructure.llm.ollama_client.OllamaClient", MockOllamaClient)
```

---

### ‚ùå Tests pasan individualmente pero fallan en suite

**S√≠ntoma**:
```bash
$ pytest tests/test_interview.py  # ‚úÖ PASS
$ pytest tests/  # ‚ùå FAIL en test_interview.py
```

**Causa**: Estado compartido entre tests

**Soluci√≥n**:

```python
import pytest

@pytest.fixture(autouse=True)
def reset_state():
    """Reset estado antes de cada test"""
    # Limpiar cache
    from app.infrastructure.cache import cache
    cache.clear()
    
    # Reset singletons
    from app.container import Container
    Container._instance = None
    
    yield
    
    # Cleanup despu√©s del test
    cache.clear()
```

---

## Problemas de Rendimiento

### üêå API responde muy lento (>5 segundos)

**Diagn√≥stico**:

```python
import time

def benchmark_endpoint(url: str, payload: dict):
    start = time.time()
    response = requests.post(url, json=payload)
    elapsed = time.time() - start
    print(f"‚è±Ô∏è {url} tom√≥ {elapsed:.2f}s")
    return response

benchmark_endpoint(
    "http://localhost:8000/api/v2/interviews/123/answer",
    {"answer": "Docker es una plataforma..."}
)
```

**Soluci√≥n 1**: Cachear embeddings

```python
# app/infrastructure/ml/embeddings_service.py
from functools import lru_cache

class EmbeddingsService:
    @lru_cache(maxsize=1000)
    def get_embedding(self, text: str):
        return self.model.encode(text)
```

**Soluci√≥n 2**: Usar modelo m√°s r√°pido

```bash
# Cambiar de llama3:8b a llama3.2:3b
export OLLAMA_MODEL=llama3.2:3b

# O incluso llama3.2:1b para m√°xima velocidad
export OLLAMA_MODEL=llama3.2:1b
```

**Soluci√≥n 3**: Reducir max_tokens

```python
# .env
OLLAMA_MAX_TOKENS=256  # En lugar de 512
```

---

### üêå Primer request es muy lento (~30 segundos)

**Causa**: Cold start - modelo se est√° cargando en memoria

**Soluci√≥n**: Warm-up al inicio

```python
# app/main_v2.py
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Warm-up: cargar modelo
    container = Container()
    llm_service = container.llm_service()
    await llm_service.generate("Hello")
    
    yield
    
    # Cleanup
    pass

app = FastAPI(lifespan=lifespan)
```

---

### üíæ Uso excesivo de memoria (>8GB)

**Diagn√≥stico**:

```bash
# Ver uso de memoria
ps aux --sort=-%mem | head -10

# Memory profiling
pip install memory_profiler
python -m memory_profiler app/main_v2.py
```

**Soluci√≥n 1**: Limitar modelos ML cargados

```python
# app/container.py
class Container:
    def __init__(self):
        # Solo cargar si es necesario
        self._emotion_detector = None
    
    def emotion_detector(self):
        if self._emotion_detector is None:
            self._emotion_detector = MultilingualEmotionDetector(device='cpu')
        return self._emotion_detector
```

**Soluci√≥n 2**: Desactivar servicios opcionales

```bash
# .env
ENABLE_STT=false
ENABLE_TTS=false
ENABLE_EMOTION_DETECTION=false
```

---

## Errores Comunes

### ‚ùå `RuntimeError: Expected all tensors to be on the same device`

**S√≠ntoma**:
```python
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0
```

**Soluci√≥n**:

```python
# Forzar CPU en todos los modelos
import torch

device = 'cpu'  # No usar 'cuda'

model = AutoModel.from_pretrained('model-name').to(device)
```

---

### ‚ùå `JSONDecodeError: Expecting value: line 1 column 1`

**S√≠ntoma**:
```python
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```

**Causa**: Respuesta vac√≠a o no-JSON de Ollama

**Soluci√≥n**:

```python
import httpx
import json

def safe_ollama_request(prompt: str):
    try:
        response = client.post("/api/generate", json={
            "model": "llama3.2:3b",
            "prompt": prompt
        })
        response.raise_for_status()
        return response.json()
    except httpx.HTTPError as e:
        print(f"‚ùå HTTP Error: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON Error: {e}")
        print(f"Raw response: {response.text}")
        return None
```

---

### ‚ùå `PromptInjectionDetected: Detected prompt injection patterns`

**S√≠ntoma**:
```json
{"detail": "Prompt injection detected: ['ignore previous instructions']"}
```

**Causa**: Input del usuario contiene patrones sospechosos

**Soluci√≥n 1**: Ajustar threshold

```python
# app/infrastructure/security/prompt_injection_guard.py
guard = PromptInjectionGuard(threshold=0.7)  # M√°s permisivo (era 0.5)
```

**Soluci√≥n 2**: Desactivar temporalmente

```bash
# .env
ENABLE_PROMPT_INJECTION_DETECTION=false
```

**Soluci√≥n 3**: Whitelist de patrones leg√≠timos

```python
ALLOWED_PATTERNS = [
    "ignore case",  # SQL
    "system design",  # Pregunta leg√≠tima
]

if any(pattern in user_input.lower() for pattern in ALLOWED_PATTERNS):
    # Permitir
    pass
```

---

## Debugging Avanzado

### üîç Activar logs detallados

```bash
# .env
LOG_LEVEL=DEBUG

# O en runtime
LOG_LEVEL=DEBUG uvicorn app.main_v2:app --reload
```

**Ver logs espec√≠ficos**:

```python
import logging

# Activar logger de Ollama
logging.getLogger("app.infrastructure.llm").setLevel(logging.DEBUG)

# Activar logger de evaluaci√≥n
logging.getLogger("app.application.services.evaluation").setLevel(logging.DEBUG)
```

---

### üîç Profiling de rendimiento

```bash
# Instalar
pip install py-spy

# Profiling en tiempo real
py-spy top -- python -m uvicorn app.main_v2:app

# Generar flamegraph
py-spy record -o profile.svg -- python -m uvicorn app.main_v2:app
```

---

### üîç Debugging de container DI

```python
# debug_container.py
from app.container import Container

container = Container()

# Ver todas las dependencias
print("üîç Verificando container...")
print(f"‚úÖ LLM Service: {container.llm_service()}")
print(f"‚úÖ Evaluation Service: {container.evaluation_service()}")
print(f"‚úÖ Question Service: {container.question_service()}")
print(f"‚úÖ Interview Service: {container.interview_service()}")

# Health check
result = container.health_check()
print(f"\nüìä Health Check: {result}")
```

---

### üîç Debugging de Ollama requests

```python
# debug_ollama.py
import httpx
import json

client = httpx.Client(base_url="http://localhost:11434")

# Test 1: List models
response = client.get("/api/tags")
print(f"üì¶ Models: {response.json()}")

# Test 2: Generate
response = client.post("/api/generate", json={
    "model": "llama3.2:3b",
    "prompt": "Say hello",
    "stream": False
})
print(f"üí¨ Response: {response.json()}")

# Test 3: Embeddings
response = client.post("/api/embeddings", json={
    "model": "llama3.2:3b",
    "prompt": "Hello world"
})
print(f"üî¢ Embeddings shape: {len(response.json()['embedding'])}")
```

---

## Obtener Ayuda

### üìù Reporte de Bug

Incluir:

1. **Versi√≥n**: `cat README.md | grep "Version"`
2. **Sistema**: `uname -a`
3. **Python**: `python --version`
4. **Logs**: `cat logs/ready4hire.log | tail -100`
5. **Configuraci√≥n**: `.env` (sin secretos)
6. **Pasos para reproducir**

### üìß Contacto

- **GitHub Issues**: [Ready4Hire/issues](https://github.com/yourusername/Ready4Hire/issues)
- **Email**: support@ready4hire.example.com
- **Discord**: [Ready4Hire Community](https://discord.gg/ready4hire)

---

## Checklist de Diagn√≥stico

Antes de reportar un bug, verificar:

- [ ] Ollama est√° corriendo: `curl http://localhost:11434/api/tags`
- [ ] Modelo descargado: `ollama list | grep llama3.2`
- [ ] Dependencias instaladas: `pip check`
- [ ] Tests pasan: `pytest tests/ -v`
- [ ] API responde: `curl http://localhost:8000/api/v2/health`
- [ ] Logs sin errores: `tail logs/ready4hire.log`
- [ ] Puerto libre: `lsof -i :8000`
- [ ] Variables de entorno: `cat .env`

---

## Problemas Resueltos

‚úÖ = Problema com√∫n ya solucionado en versi√≥n actual

- ‚úÖ ModuleNotFoundError al iniciar API
- ‚úÖ Tests fallan por falta de Ollama
- ‚úÖ CORS bloqueando frontend
- ‚úÖ Primer request muy lento (cold start)
- ‚úÖ Prompt injection demasiado estricto

---

## Conclusi√≥n

Si despu√©s de revisar esta gu√≠a el problema persiste:

1. **Activar DEBUG**: `LOG_LEVEL=DEBUG`
2. **Revisar logs**: `tail -f logs/ready4hire.log`
3. **Ejecutar tests**: `pytest tests/ -v`
4. **Health check**: `curl http://localhost:8000/api/v2/health`
5. **Reportar issue** con toda la informaci√≥n

¬°Buena suerte! üöÄ
